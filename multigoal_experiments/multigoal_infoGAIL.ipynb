{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Goal Imitation Learning\n"
     ]
    }
   ],
   "source": [
    "import warnings, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import scipy.signal\n",
    "import pickle\n",
    "\n",
    "import multigoal\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Multi Goal Imitation Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyLatent(object):\n",
    "    def __init__(self, obs_dim, act_dim, latent_dim=4, clip_range=0.2,\n",
    "                 epochs=10, lr=3e-5, hdim=64, max_std=1.0,\n",
    "                 beta=1.0, eta=100, kl_targ=0.003,entcoeff=1e-3, logpostcoeff=1e-2,\n",
    "                 seed=0):\n",
    "        \n",
    "        self.seed=0\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        self.kl_targ = kl_targ\n",
    "        self.entcoeff = entcoeff\n",
    "        self.logpostcoeff = logpostcoeff\n",
    "#         self.clip_range = clip_range\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.hdim = hdim\n",
    "        self.max_std = max_std\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._init_session()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self._placeholders()\n",
    "            self._policy_nn()\n",
    "            self._logprob()\n",
    "            self._kl_entropy()\n",
    "            self._loss_train_op()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "            \n",
    "    def _placeholders(self):\n",
    "        # observations, actions and advantages:\n",
    "        self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs')\n",
    "        self.act_ph = tf.placeholder(tf.float32, (None, self.act_dim), 'act')\n",
    "        self.latent_ph = tf.placeholder(tf.float32, (None, self.latent_dim), 'latent')\n",
    "        self.logpost_ph = tf.placeholder(tf.float32, (None, ), 'logpost')\n",
    "        self.advantages_ph = tf.placeholder(tf.float32, (None,), 'advantages')\n",
    "\n",
    "        # strength of D_KL loss terms:\n",
    "        self.beta_ph = tf.placeholder(tf.float32, (), 'beta')\n",
    "        self.eta_ph = tf.placeholder(tf.float32, (), 'eta')\n",
    "        \n",
    "        # learning rate:\n",
    "        self.lr_ph = tf.placeholder(tf.float32, (), 'lr')\n",
    "        \n",
    "        # place holder for old parameters\n",
    "        self.old_std_ph = tf.placeholder(tf.float32, (None, self.act_dim), 'old_std')\n",
    "        self.old_mean_ph = tf.placeholder(tf.float32, (None, self.act_dim), 'old_means')\n",
    "\n",
    "    def _policy_nn(self):\n",
    "        \n",
    "        hid1_size = self.hdim\n",
    "        hid2_size = self.hdim\n",
    "        \n",
    "        # TWO HIDDEN LAYERS\n",
    "        out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                              kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h1\")\n",
    "        out = tf.layers.dense(tf.concat([out, self.latent_ph],axis=1), hid2_size, tf.tanh,\n",
    "                              kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h2\")\n",
    "                \n",
    "        # MEAN FUNCTION\n",
    "        self.mean = tf.layers.dense(out, self.act_dim,\n",
    "                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), \n",
    "                                name=\"mean\")\n",
    "        # UNIT VARIANCE\n",
    "        self.logits_std = tf.get_variable(\"logits_std\",shape=(1,),initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed))\n",
    "        self.std = self.max_std*tf.ones_like(self.mean)*tf.sigmoid(self.logits_std)\n",
    "        \n",
    "        # SAMPLE OPERATION\n",
    "        self.sample_action = self.mean + tf.random_normal(tf.shape(self.mean),seed= self.seed)*self.std\n",
    "        \n",
    "    def _logprob(self):\n",
    "        # PROBABILITY WITH TRAINING PARAMETER\n",
    "        y = self.act_ph \n",
    "        mu = self.mean\n",
    "        sigma = self.std\n",
    "        \n",
    "        self.logp = tf.reduce_sum(-0.5*tf.square((y-mu)/sigma)-tf.log(sigma)- 0.5*np.log(2.*np.pi),axis=1)\n",
    "\n",
    "        # PROBABILITY WITH OLD (PREVIOUS) PARAMETER\n",
    "        old_mu_ph = self.old_mean_ph\n",
    "        old_sigma_ph = self.old_std_ph\n",
    "                \n",
    "        self.logp_old = tf.reduce_sum(-0.5*tf.square((y-old_mu_ph)/old_sigma_ph)-tf.log(old_sigma_ph)- 0.5*np.log(2.*np.pi),axis=1)\n",
    "        \n",
    "    def _kl_entropy(self):\n",
    "\n",
    "        mean, std = self.mean, self.std\n",
    "        old_mean, old_std = self.old_mean_ph, self.old_std_ph\n",
    " \n",
    "        log_std_old = tf.log(old_std)\n",
    "        log_std_new = tf.log(std)\n",
    "        frac_std_old_new = old_std/std\n",
    "\n",
    "        # KL DIVERGENCE BETWEEN TWO GAUSSIAN\n",
    "        kl = tf.reduce_sum(log_std_new - log_std_old + 0.5*tf.square(frac_std_old_new) + 0.5*tf.square((mean - old_mean)/std)- 0.5,axis=1)\n",
    "        self.kl = tf.reduce_mean(kl)\n",
    "        \n",
    "        # ENTROPY OF GAUSSIAN\n",
    "        entropy = tf.reduce_sum(log_std_new + 0.5 + 0.5*np.log(2*np.pi),axis=1)\n",
    "        self.entropy = tf.reduce_mean(entropy)\n",
    "        \n",
    "    def _loss_train_op(self):\n",
    "        \n",
    "        # Proximal Policy Optimization CLIPPED LOSS FUNCTION\n",
    "#         ratio = tf.exp(self.logp - self.logp_old) \n",
    "#         clipped_ratio = tf.clip_by_value(ratio,clip_value_min=1-self.clip_range,clip_value_max=1+self.clip_range) \n",
    "#         self.loss = -tf.reduce_mean(tf.minimum(self.advantages_ph*ratio,self.advantages_ph*clipped_ratio))\n",
    "        \n",
    "        loss1 = -tf.reduce_mean((self.advantages_ph + self.logpostcoeff*self.logpost_ph) * tf.exp(self.logp - self.logp_old))\n",
    "        loss2 = tf.reduce_mean(self.beta_ph * self.kl)\n",
    "        loss3 = self.eta_ph * tf.square(tf.maximum(0.0, self.kl - 2.0 * self.kl_targ))\n",
    "        self.loss = loss1 - self.entcoeff*self.entropy + loss2 + loss3\n",
    "        \n",
    "        # OPTIMIZER \n",
    "        optimizer = tf.train.AdamOptimizer(self.lr_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config,graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def sample(self, obs, latent): # SAMPLE FROM POLICY        \n",
    "        feed_dict = {self.obs_ph: obs, self.latent_ph:latent}\n",
    "        sampled_action = self.sess.run(self.sample_action,feed_dict=feed_dict)\n",
    "        return sampled_action\n",
    "    \n",
    "    def control(self, obs, latent): # COMPUTE MEAN\n",
    "        feed_dict = {self.obs_ph: obs, self.latent_ph:latent}\n",
    "        best_action = self.sess.run(self.mean,feed_dict=feed_dict)\n",
    "        return best_action        \n",
    "    \n",
    "    def update(self, observes, actions, latents, logposts, advantages, batch_size = 128): # TRAIN POLICY\n",
    "        \n",
    "        num_batches = max(observes.shape[0] // batch_size, 1)\n",
    "        batch_size = observes.shape[0] // num_batches\n",
    "        \n",
    "        old_means_np, old_std_np = self.sess.run([self.mean, self.std],{self.obs_ph: observes,self.latent_ph:latents}) # COMPUTE OLD PARAMTER\n",
    "        for e in range(self.epochs):\n",
    "            observes, actions, latents, logposts, advantages, old_means_np, old_std_np = shuffle(observes, actions, latents, logposts, advantages, old_means_np, old_std_np, random_state=self.seed)\n",
    "            for j in range(num_batches): \n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: observes[start:end,:],\n",
    "                     self.act_ph: actions[start:end,:],\n",
    "                     self.latent_ph: latents[start:end,:],\n",
    "                     self.logpost_ph: logposts[start:end],\n",
    "                     self.advantages_ph: advantages[start:end],\n",
    "                     self.old_std_ph: old_std_np[start:end,:],\n",
    "                     self.old_mean_ph: old_means_np[start:end,:],\n",
    "                     self.beta_ph: self.beta,\n",
    "                     self.eta_ph: self.eta,\n",
    "                     self.lr_ph: self.lr}        \n",
    "                self.sess.run(self.train_op, feed_dict)\n",
    "            \n",
    "        feed_dict = {self.obs_ph: observes,\n",
    "                 self.act_ph: actions,\n",
    "                 self.latent_ph: latents,\n",
    "                 self.logpost_ph: logposts,\n",
    "                 self.advantages_ph: advantages,\n",
    "                 self.old_std_ph: old_std_np,\n",
    "                 self.old_mean_ph: old_means_np,\n",
    "                 self.beta_ph: self.beta,\n",
    "                 self.eta_ph: self.eta,\n",
    "                 self.lr_ph: self.lr}             \n",
    "        loss, kl, entropy = self.sess.run([self.loss, self.kl, self.entropy], feed_dict)\n",
    "        return loss, kl, entropy\n",
    "    \n",
    "    def close_sess(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentPosterior(object):\n",
    "    def __init__(self, obs_dim, act_dim, latent_dim=4, epochs=20, lr=1e-3, hdim=64, seed=0):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.hdim = hdim\n",
    "        self.seed = seed\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\" Construct TensorFlow graph, including loss function, init op and train op \"\"\"\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            # Place Holder for \n",
    "            self.obs_act_gen_ph = tf.placeholder(tf.float32, (None, self.obs_dim + self.act_dim), 'obs_act_gen_rewfunc')\n",
    "            self.latent_ph = tf.placeholder(tf.float32, (None, self.latent_dim), 'latent_gen_rewfunc')\n",
    "            \n",
    "            hid1_size = self.hdim\n",
    "            hid2_size = self.hdim\n",
    "            \n",
    "            # Network for expert demonstrations\n",
    "            out = tf.layers.dense(self.obs_act_gen_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\")\n",
    "            gen_logits = tf.layers.dense(out, self.latent_dim,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output')\n",
    "            self.gen_logits = gen_logits\n",
    "            self.gen_prob = tf.nn.softmax(gen_logits)\n",
    "            \n",
    "            # Loss for classification\n",
    "            generator_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.gen_logits, labels=self.latent_ph)\n",
    "            generator_loss = tf.reduce_mean(generator_loss)\n",
    "            \n",
    "            # Total loss\n",
    "            self.loss = generator_loss\n",
    "                        \n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "            \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config,graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def fit(self, latent, obs_act_gen, batch_size=128):\n",
    "        data_size = obs_act_gen.shape[0]\n",
    "        num_batches = max(data_size // batch_size, 1)\n",
    "        batch_size = data_size // num_batches\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            obs_act_gen, latent = shuffle(obs_act_gen, latent, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_act_gen_ph:obs_act_gen[start:end,:], self.latent_ph:latent[start:end,:]}\n",
    "                self.sess.run(self.train_op, feed_dict=feed_dict)\n",
    "        feed_dict = {self.obs_act_gen_ph:obs_act_gen, self.latent_ph:latent}\n",
    "        loss = self.sess.run(self.loss, feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, x): # Predict the rewards\n",
    "        feed_dict = {self.obs_act_gen_ph: x}\n",
    "        gen_prob = self.sess.run(self.gen_prob, feed_dict=feed_dict)\n",
    "        latent_code = np.zeros([1,self.latent_dim])\n",
    "        latent_code[np.random.choice(1,p=gen_prob)] = 1\n",
    "        return latent_code\n",
    "    \n",
    "    def logpost(self, x, latent): # Predict the rewards\n",
    "        feed_dict = {self.obs_act_gen_ph: x}\n",
    "        gen_prob = self.sess.run(self.gen_prob, feed_dict=feed_dict)\n",
    "        logpost = np.sum(np.log(gen_prob)*latent,axis=1)\n",
    "        return logpost\n",
    "        \n",
    "    def close_sess(self):\n",
    "        self.sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(object):\n",
    "    def __init__(self, obs_dim, epochs=20, lr=1e-4, hdim=64, seed=0):\n",
    "        self.seed = seed\n",
    "    \n",
    "        self.obs_dim = obs_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.hdim = hdim\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._init_session()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs_valfunc')\n",
    "            self.val_ph = tf.placeholder(tf.float32, (None,), 'val_valfunc')\n",
    "            \n",
    "            hid1_size = self.hdim \n",
    "            hid2_size = self.hdim \n",
    "            \n",
    "            out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\")\n",
    "            out = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output')\n",
    "            self.out = tf.squeeze(out)\n",
    "            \n",
    "            # L2 LOSS\n",
    "            self.loss = tf.reduce_mean(tf.square(self.out - self.val_ph))\n",
    "            \n",
    "            # OPTIMIZER\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "    \n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config,graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def fit(self, x, y, batch_size=32):\n",
    "        num_batches = max(x.shape[0] // batch_size, 1)\n",
    "        x_train, y_train = x, y\n",
    "        for e in range(self.epochs):\n",
    "            x_train, y_train = shuffle(x_train, y_train, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: x_train[start:end, :],\n",
    "                             self.val_ph: y_train[start:end]}\n",
    "                self.sess.run([self.train_op], feed_dict=feed_dict)\n",
    "        feed_dict = {self.obs_ph: x_train,\n",
    "                     self.val_ph: y_train}\n",
    "        loss, = self.sess.run([self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x): # PREDICT VALUE OF THE GIVEN STATE\n",
    "        feed_dict = {self.obs_ph: x}\n",
    "        y_hat = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "        return np.squeeze(y_hat)\n",
    "\n",
    "    def close_sess(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward(object):\n",
    "    def __init__(self, obs_dim, act_dim, epochs=10, hdim=32, lr=1e-3, entcoeff=1e-1, seed=0):\n",
    "        self.replay_buffer_obs_act_gen = None\n",
    "        self.seed = seed\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.hdim = hdim\n",
    "        self.epochs = epochs\n",
    "        self.entcoeff = entcoeff # Heuristics\n",
    "        self.lr = lr\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\" Construct TensorFlow graph, including loss function, init op and train op \"\"\"\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            # Place Holder for \n",
    "            self.obs_act_exp_ph = tf.placeholder(tf.float32, (None, self.obs_dim + self.act_dim), 'obs_act_exp_rewfunc')\n",
    "            self.obs_act_gen_ph = tf.placeholder(tf.float32, (None, self.obs_dim + self.act_dim), 'obs_act_gen_rewfunc')\n",
    "            \n",
    "            hid1_size = self.hdim\n",
    "            hid2_size = self.hdim\n",
    "            \n",
    "            # Network for expert demonstrations\n",
    "            out = tf.layers.dense(self.obs_act_exp_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\")\n",
    "            exp_logits = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output')\n",
    "            self.exp_logits = tf.squeeze(exp_logits)\n",
    "            \n",
    "            # Network for learner's demonstrations. Use the same parameter which is defined above\n",
    "            out = tf.layers.dense(self.obs_act_gen_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\", reuse=True) # Reuse=Ture -> use the same parameter\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\", reuse=True) # Reuse=Ture -> use the same parameter\n",
    "            gen_logits = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output', reuse=True) # Reuse=Ture -> use the same parameter\n",
    "            self.gen_logits = tf.squeeze(gen_logits)\n",
    "\n",
    "            # Check accuracy\n",
    "            self.generator_acc = tf.reduce_mean(tf.to_float(gen_logits > 0.0))\n",
    "            self.expert_acc = tf.reduce_mean(tf.to_float(exp_logits < 0.0))\n",
    "            \n",
    "            # Loss for classification\n",
    "#             generator_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=gen_logits, labels=tf.ones_like(gen_logits))\n",
    "            \n",
    "            generator_loss = tf.reduce_mean(gen_logits)\n",
    "            expert_loss = -tf.reduce_mean(exp_logits)\n",
    "            \n",
    "            # Entropy regularization\n",
    "            logits = tf.concat([gen_logits, exp_logits], 0)\n",
    "            entropy = tf.reduce_mean(logits**2) \n",
    "            entropy_loss = self.entcoeff*entropy\n",
    "            \n",
    "            # Total loss\n",
    "            self.loss = generator_loss + expert_loss + entropy_loss\n",
    "            \n",
    "            # Build Reward for policy\n",
    "            self.reward = gen_logits\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            gvs = optimizer.compute_gradients(self.loss)\n",
    "            clipped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "            self.train_op = optimizer.apply_gradients(clipped_gvs)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "        self.sess = tf.Session(graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def fit(self, obs_act_exp, obs_act_gen, batch_size=128):\n",
    "        data_size = min(obs_act_exp.shape[0],obs_act_gen.shape[0])\n",
    "        num_batches = max(data_size // batch_size, 1)\n",
    "        batch_size = data_size // num_batches\n",
    "        \n",
    "        obs_act_exp_train = obs_act_exp\n",
    "        if self.replay_buffer_obs_act_gen is None:\n",
    "            obs_act_gen_train = obs_act_gen\n",
    "        else:\n",
    "            obs_act_gen_train = np.concatenate([obs_act_gen, self.replay_buffer_obs_act_gen])\n",
    "        self.replay_buffer_obs_act_gen = obs_act_gen\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            obs_act_exp_train = shuffle(obs_act_exp_train, random_state=self.seed)\n",
    "            obs_act_gen_train = shuffle(obs_act_gen_train, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_act_gen_ph:obs_act_gen_train[start:end,:], self.obs_act_exp_ph:obs_act_exp_train[start:end,:]}\n",
    "                self.sess.run(self.train_op, feed_dict=feed_dict)\n",
    "        feed_dict = {self.obs_act_gen_ph:obs_act_gen_train, self.obs_act_exp_ph:obs_act_exp_train}\n",
    "        loss, gen_acc, exp_acc = self.sess.run([self.loss,self.generator_acc,self.expert_acc], feed_dict=feed_dict)\n",
    "        return loss, gen_acc, exp_acc\n",
    "    \n",
    "    def predict(self, x): # Predict the rewards\n",
    "        feed_dict = {self.obs_act_gen_ph: x}\n",
    "        rew_hat = self.sess.run(self.reward, feed_dict=feed_dict)\n",
    "        return np.squeeze(rew_hat)\n",
    "        \n",
    "    def close_sess(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
    "\n",
    "\n",
    "def add_disc_sum_rew(trajectories, gamma):\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        disc_sum_rew = discount(rewards, gamma)\n",
    "        trajectory['disc_sum_rew'] = disc_sum_rew\n",
    "\n",
    "def add_rew(trajectories, rew_func):\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        actions = trajectory['actions']\n",
    "        observes_actions = np.concatenate([observes,actions],axis=1)\n",
    "        trajectory['rewards'] = rew_func.predict(observes_actions)\n",
    "    return trajectories\n",
    "\n",
    "def add_value(trajectories, val_func):\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = val_func.predict(observes)\n",
    "        trajectory['values'] = values\n",
    "\n",
    "def add_gae(trajectories, gamma, lam):\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        values = trajectory['values']\n",
    "        # temporal differences\n",
    "        tds = rewards - values + np.append(values[1:] * gamma, 0)\n",
    "        advantages = discount(tds, gamma * lam)\n",
    "        trajectory['advantages'] = advantages\n",
    "\n",
    "def build_train_set(trajectories):\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    latents = np.concatenate([t['latents'] for t in trajectories])\n",
    "    disc_sum_rew = np.concatenate([t['disc_sum_rew'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "    # normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "\n",
    "    return observes, actions, latents, advantages, disc_sum_rew\n",
    "\n",
    "def build_train_set_for_rew(trajectories,demonstrations):\n",
    "    \n",
    "    real_observes = np.concatenate([d['observes'] for d in demonstrations])\n",
    "    real_actions = np.concatenate([d['actions'] for d in demonstrations])\n",
    "    obs_act_exp = np.concatenate([real_observes,real_actions],axis=1)\n",
    "    \n",
    "    fake_observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    fake_actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    obs_act_gen = np.concatenate([fake_observes,fake_actions],axis=1)\n",
    "    return obs_act_exp, obs_act_gen\n",
    "\n",
    "def run_episode(env, policy, animate=False):\n",
    "    obs = env.reset()\n",
    "    observes, actions, latents, rewards, infos = [], [], [], [], []\n",
    "    done = False\n",
    "    latent = np.zeros([1,policy.latent_dim])\n",
    "    latent[0][np.random.choice(policy.latent_dim)] = 1\n",
    "    while not done:\n",
    "        if animate:\n",
    "            env.render()\n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        observes.append(obs)\n",
    "        action = policy.sample(obs,latent).reshape((1, -1)).astype(np.float32)\n",
    "        actions.append(action)\n",
    "        latents.append(latent)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if not isinstance(reward, float):\n",
    "            reward = np.asscalar(reward)\n",
    "        rewards.append(reward)\n",
    "        infos.append(info['goal_id'])\n",
    "        \n",
    "    return (np.concatenate(observes), np.concatenate(actions), np.concatenate(latents),\n",
    "            np.array(rewards, dtype=np.float32), infos)\n",
    "\n",
    "def run_policy(env, policy, episodes):\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, latents, rewards, infos = run_episode(env, policy)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes,\n",
    "                      'actions': actions,\n",
    "                      'latents': latents,\n",
    "                      'true_rewards': rewards,\n",
    "                      'infos': infos}\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "\n",
    "def evaluation(env, policy, max_eval_epi=100, seed=0):\n",
    "    return_list = np.zeros((max_eval_epi,))\n",
    "    info_list = []\n",
    "\n",
    "    env.seed(seed)\n",
    "    for epi in range(max_eval_epi):\n",
    "        obs = env.reset()\n",
    "        env_infos_epi_list = {\"pos\": [],\"goal_id\": []}\n",
    "        observes, actions, rewards, infos = run_episode(env, policy)\n",
    "\n",
    "        pos_list = []\n",
    "        goal_id_list = []\n",
    "        for info in infos:\n",
    "            pos_list.append(info[\"pos\"])\n",
    "            goal_id_list.append(info[\"goal_id\"])\n",
    "        env_infos_epi_list[\"pos\"] = np.asarray(pos_list)\n",
    "        env_infos_epi_list[\"goal_id\"] = np.asarray(goal_id_list)\n",
    "\n",
    "        info_list.append({\"env_infos\":env_infos_epi_list})\n",
    "        return_list[epi] = np.sum(rewards)\n",
    "#     print(\"Evaluation Result: {}\".format(np.mean(return_list)))\n",
    "    return return_list, info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/300] True Mean Ret : -48.349, Train Mean Ret : -25.790, Reward Loss : -0.935, Reward Acc : [0.058,0.527], Posterior : 0.560, Value Loss : 0.002, Policy loss : -0.01474, Policy KL : 0.00901, Policy Entropy : 0.075 ***\n",
      "[20/300] True Mean Ret : 145.551, Train Mean Ret : -342.115, Reward Loss : -63.339, Reward Acc : [0.295,0.000], Posterior : 0.557, Value Loss : 0.440, Policy loss : -0.02188, Policy KL : 0.00992, Policy Entropy : -0.181 ***\n",
      "[40/300] True Mean Ret : -102.539, Train Mean Ret : 729.745, Reward Loss : -30.356, Reward Acc : [0.704,0.069], Posterior : 0.541, Value Loss : 1.500, Policy loss : -0.00298, Policy KL : 0.00623, Policy Entropy : -0.909 ***\n",
      "[60/300] True Mean Ret : -112.415, Train Mean Ret : 1642.067, Reward Loss : 0.900, Reward Acc : [0.960,0.002], Posterior : 0.520, Value Loss : 1.096, Policy loss : -0.00078, Policy KL : 0.00492, Policy Entropy : -1.938 ***\n",
      "[80/300] True Mean Ret : 118.229, Train Mean Ret : 853.759, Reward Loss : 1.208, Reward Acc : [0.994,0.000], Posterior : 0.545, Value Loss : 0.192, Policy loss : 0.00272, Policy KL : 0.00117, Policy Entropy : -2.336 ***\n",
      "[100/300] True Mean Ret : 5.539, Train Mean Ret : 258.559, Reward Loss : -0.092, Reward Acc : [0.991,0.000], Posterior : 0.528, Value Loss : 0.031, Policy loss : 0.00364, Policy KL : 0.00208, Policy Entropy : -2.514 ***\n",
      "[120/300] True Mean Ret : 105.947, Train Mean Ret : 84.991, Reward Loss : -0.061, Reward Acc : [0.995,0.000], Posterior : 0.551, Value Loss : 0.004, Policy loss : 0.00494, Policy KL : 0.00290, Policy Entropy : -2.770 ***\n",
      "[140/300] True Mean Ret : 130.176, Train Mean Ret : 55.216, Reward Loss : -0.042, Reward Acc : [0.996,0.001], Posterior : 0.538, Value Loss : 0.002, Policy loss : 0.00422, Policy KL : 0.00232, Policy Entropy : -3.040 ***\n",
      "[160/300] True Mean Ret : 140.175, Train Mean Ret : 19.631, Reward Loss : -0.015, Reward Acc : [0.999,0.000], Posterior : 0.552, Value Loss : 0.000, Policy loss : 0.00701, Policy KL : 0.00670, Policy Entropy : -3.375 ***\n",
      "[180/300] True Mean Ret : -31.413, Train Mean Ret : 9.208, Reward Loss : -0.007, Reward Acc : [0.999,0.001], Posterior : 0.541, Value Loss : 0.000, Policy loss : 0.00444, Policy KL : 0.00206, Policy Entropy : -3.735 ***\n",
      "[200/300] True Mean Ret : 81.601, Train Mean Ret : 1.392, Reward Loss : -0.001, Reward Acc : [1.000,0.000], Posterior : 0.538, Value Loss : 0.000, Policy loss : 0.00655, Policy KL : 0.00276, Policy Entropy : -3.902 ***\n",
      "[220/300] True Mean Ret : -25.010, Train Mean Ret : 1.485, Reward Loss : -0.003, Reward Acc : [1.000,0.001], Posterior : 0.535, Value Loss : 0.000, Policy loss : 0.00688, Policy KL : 0.00493, Policy Entropy : -3.992 ***\n",
      "[240/300] True Mean Ret : 3.643, Train Mean Ret : -0.005, Reward Loss : 0.000, Reward Acc : [0.000,1.000], Posterior : 0.529, Value Loss : 0.000, Policy loss : 0.00460, Policy KL : 0.00348, Policy Entropy : -4.005 ***\n",
      "[260/300] True Mean Ret : -52.653, Train Mean Ret : 1.273, Reward Loss : -0.003, Reward Acc : [1.000,0.000], Posterior : 0.528, Value Loss : 0.000, Policy loss : 0.01001, Policy KL : 0.00567, Policy Entropy : -4.080 ***\n",
      "[280/300] True Mean Ret : -124.460, Train Mean Ret : -0.007, Reward Loss : 0.000, Reward Acc : [0.000,1.000], Posterior : 0.527, Value Loss : 0.000, Policy loss : 0.00333, Policy KL : 0.00233, Policy Entropy : -4.173 ***\n",
      "[300/300] True Mean Ret : -53.125, Train Mean Ret : -0.006, Reward Loss : 0.000, Reward Acc : [0.000,1.000], Posterior : 0.530, Value Loss : 0.000, Policy loss : 0.00800, Policy KL : 0.00345, Policy Entropy : -4.285 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception TypeError: \"render() got an unexpected keyword argument 'close'\" in <bound method MultiGoalEnv.__del__ of <multigoal.MultiGoalEnv object at 0x7fcfe2607390>> ignored\n"
     ]
    }
   ],
   "source": [
    "def train_info_imitation_learning(seed,entcoeff,logpostcoeff,n_mixture,demo_size,GPU_ID=0,kl_targ=0.003,\n",
    "                             gamma = 0.995,lam = 0.98,max_std=0.5,episode_size = 500,batch_size = 512,\n",
    "                             nupdates = 300,save_iter=100,min_save_iter=200,verbose=False):\n",
    "    np.random.seed = seed\n",
    "    tf.set_random_seed(seed)\n",
    "    demo_file = open('./multigoal_expert_demo.pkl', 'r')\n",
    "    demonstrations, = pickle.load(demo_file)\n",
    "    demonstrations = shuffle(demonstrations,random_state=seed)[:demo_size]\n",
    "\n",
    "    demo_observes = []\n",
    "    demo_actions = []\n",
    "    for demonstration in demonstrations:\n",
    "        for obs in demonstration['observes']:\n",
    "            demo_observes.append(obs)\n",
    "        for act in demonstration['actions']:\n",
    "            demo_actions.append(act)\n",
    "    demo_observes=np.asarray(demo_observes)\n",
    "    demo_actions=np.asarray(demo_actions)\n",
    "    exp_ret = np.mean([np.sum(t['rewards']) for t in demonstrations])\n",
    "\n",
    "    env = multigoal.MultiGoalEnv(nr_goal=4)\n",
    "    env.seed(seed)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Define three networks\n",
    "    posterior = LatentPosterior(obs_dim, act_dim, latent_dim=n_mixture, epochs=20, lr=1e-3)\n",
    "    policy = PolicyLatent(obs_dim, act_dim, latent_dim=n_mixture, max_std=max_std, entcoeff=entcoeff,logpostcoeff=logpostcoeff, epochs=20, hdim=64, lr=1e-3, clip_range=0.2, seed=seed)\n",
    "    val_func = Value(obs_dim, epochs=20, hdim=64, lr=1e-3, seed=seed)\n",
    "    rew_func = Reward(obs_dim, act_dim, epochs=20, hdim=32, lr=3e-4, seed=seed)\n",
    "    mean_ret_list = []\n",
    "    info_list = []\n",
    "    \n",
    "    saver_prefix=\"./results_info_gail/seed:{},kl:{:.2e},entcoeff:{:.2e},logpostcoeff:{:.2e},mixture:{:d},epi_size:{}\".format(seed,kl_targ,entcoeff,logpostcoeff,n_mixture,episode_size)\n",
    "\n",
    "    n_episodes = episode_size\n",
    "    train_rewards = True\n",
    "    for update in range(nupdates+1):\n",
    "\n",
    "        # Generate data\n",
    "        trajectories = run_policy(env, policy, episodes=n_episodes)\n",
    "\n",
    "        # Build data set for training rewards function\n",
    "        obs_act_exp, obs_act_gen = build_train_set_for_rew(trajectories,demonstrations) \n",
    "        rew_loss, gen_acc, exp_acc = rew_func.fit(obs_act_exp, obs_act_gen, batch_size=batch_size)\n",
    "\n",
    "        add_rew(trajectories,rew_func)\n",
    "        add_value(trajectories, val_func)  # add estimated values to episodes\n",
    "        add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n",
    "        add_gae(trajectories, gamma, lam)  # calculate advantage\n",
    "        # Build data set for training policy and value function\n",
    "        observes, actions, latents, advantages, returns = build_train_set(trajectories)\n",
    "        logposts = posterior.logpost(obs_act_gen,latents)\n",
    "\n",
    "        # Train policy and value\n",
    "        pol_loss, pol_kl, pol_entropy = policy.update(observes, actions, latents, logposts, advantages, batch_size=batch_size)  # update policy\n",
    "        vf_loss = val_func.fit(observes, returns, batch_size=batch_size)  # update value function\n",
    "        post_loss = posterior.fit(latents, obs_act_gen,batch_size=batch_size)\n",
    "        \n",
    "        mean_ret = np.mean([np.sum(t['true_rewards']) for t in trajectories])\n",
    "        mean_ret_list.append(mean_ret)\n",
    "        info_list.append([t['infos'] for t in trajectories])\n",
    "        \n",
    "        train_ret = np.mean([np.sum(t['rewards']) for t in trajectories])\n",
    "        if (update%20) == 0 and verbose:\n",
    "            print('[{}/{}] True Mean Ret : {:.3f}, Train Mean Ret : {:.3f}, Reward Loss : {:.3f}, Reward Acc : [{:.3f},{:.3f}], Posterior : {:.3f}, Value Loss : {:.3f}, Policy loss : {:.5f}, Policy KL : {:.5f}, Policy Entropy : {:.3f} ***'.\n",
    "                  format(update, nupdates, mean_ret, train_ret, rew_loss, gen_acc, exp_acc, post_loss, vf_loss, pol_loss, pol_kl, pol_entropy))\n",
    "    \n",
    "    evaluation_results = { \"mean_ret_list\": mean_ret_list, \"info_list\": info_list }\n",
    "    pickle.dump( evaluation_results, open( saver_prefix+\".pickle\", \"wb\" ) )\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    seed=0\n",
    "    entcoeff=1e-3\n",
    "    logpostcoeff=1e-3\n",
    "    demo_size=300\n",
    "    n_mixture=4\n",
    "    train_info_imitation_learning(seed,entcoeff,logpostcoeff,n_mixture,demo_size,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
