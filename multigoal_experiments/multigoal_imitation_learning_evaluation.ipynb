{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import scipy.signal\n",
    "import pickle\n",
    "import multigoal\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, obs_dim, act_dim, \n",
    "                 beta=1.0, eta=100, kl_targ=0.003, entcoeff=1e-3,\n",
    "                 epochs=20, lr=3e-4,\n",
    "                 mdn_weight=\"softmax\", n_mixture=4, max_std=0.5, eps=0.09,\n",
    "                 seed=0):\n",
    "        \n",
    "        self.seed=0\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        self.kl_targ = kl_targ\n",
    "        self.entcoeff = entcoeff\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.lr_multiplier = 1.0\n",
    "                \n",
    "        self.mdn_weight = mdn_weight\n",
    "        self.n_mixture = n_mixture\n",
    "        self.max_std = max_std\n",
    "        self.eps = eps\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._init_session()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self._placeholders()\n",
    "            self._policy_nn()\n",
    "            self._logprob()\n",
    "            self._kl_entropy()\n",
    "            self._loss_train_op()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "            \n",
    "    def _placeholders(self):\n",
    "        # observations, actions and advantages:\n",
    "        self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs')\n",
    "        self.act_ph = tf.placeholder(tf.float32, (None, self.act_dim), 'act')\n",
    "        self.advantages_ph = tf.placeholder(tf.float32, (None,), 'advantages')\n",
    "        self.eps_ph = tf.placeholder(tf.float32, (), 'epsilon')\n",
    "        \n",
    "        # strength of D_KL loss terms:\n",
    "        self.beta_ph = tf.placeholder(tf.float32, (), 'beta')\n",
    "        self.eta_ph = tf.placeholder(tf.float32, (), 'eta')\n",
    "        \n",
    "        # learning rate:\n",
    "        self.lr_ph = tf.placeholder(tf.float32, (), 'lr')\n",
    "        \n",
    "        self.old_std_ph = tf.placeholder(tf.float32, (None, self.act_dim, self.n_mixture), 'old_std')\n",
    "        self.old_means_ph = tf.placeholder(tf.float32, (None, self.act_dim, self.n_mixture), 'old_means')\n",
    "        self.old_pi_ph = tf.placeholder(tf.float32, (None, self.n_mixture), 'old_pi')\n",
    "    \n",
    "    def _policy_nn(self):\n",
    "        \n",
    "        hid1_size = 64  \n",
    "        hid2_size = 64\n",
    "        \n",
    "        out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                              kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h1\")\n",
    "        out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                              kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h2\")\n",
    "        means = tf.layers.dense(out, self.act_dim*self.n_mixture,\n",
    "                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), \n",
    "                                name=\"flat_means\")\n",
    "        self.means = tf.reshape(means,shape=[-1,self.act_dim,self.n_mixture], name=\"means\")\n",
    "        logits_std = tf.layers.dense(out, self.act_dim*self.n_mixture,\n",
    "                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), \n",
    "                                name=\"flat_logits_std\")\n",
    "        self.std = tf.reshape(self.max_std*tf.sigmoid(logits_std),shape=[-1,self.act_dim,self.n_mixture], name=\"std\")\n",
    "        self.std = self.std + self.eps_ph\n",
    "        if self.mdn_weight==\"softmax\":\n",
    "            self.pi = tf.nn.softmax(tf.layers.dense(out, self.n_mixture,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"pi\"))\n",
    "        elif self.mdn_weight==\"sparsemax\":\n",
    "            self.pi = tf.contrib.sparsemax.sparsemax(tf.layers.dense(out, self.n_mixture,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"pi\"))\n",
    "    def _logprob(self):\n",
    "        y = self.act_ph \n",
    "        mu = self.means\n",
    "        sigma = self.std + 1e-10\n",
    "        pi = self.pi + 1e-10\n",
    "        \n",
    "        quadratics = -0.5*tf.reduce_sum(tf.square((tf.tile(y[:,:,tf.newaxis],[1,1,self.n_mixture])-mu)/sigma),axis=1)\n",
    "        logdet = -0.5*tf.reduce_sum(tf.log(sigma),axis=1)\n",
    "        logconstant = - 0.5*self.act_dim*np.log(2.*np.pi)\n",
    "        logpi = tf.log(pi)\n",
    "        \n",
    "        exponents = quadratics + logdet + logconstant + logpi\n",
    "        logprobs = tf.reduce_logsumexp(exponents,axis=1)\n",
    "        \n",
    "        self.logp = logprobs\n",
    "\n",
    "        old_mu_ph = self.old_means_ph\n",
    "        old_sigma_ph = self.old_std_ph + 1e-10\n",
    "        old_pi_ph = self.old_pi_ph + 1e-10\n",
    "    \n",
    "        quadratics = -0.5*tf.reduce_sum(tf.square((tf.tile(y[:,:,tf.newaxis],[1,1,self.n_mixture])-old_mu_ph)/old_sigma_ph),axis=1)\n",
    "        logdet = -0.5*tf.reduce_sum(tf.log(old_sigma_ph),axis=1)\n",
    "        logconstant = - 0.5*self.act_dim*np.log(2.*np.pi)\n",
    "        logpi = tf.log(old_pi_ph)\n",
    "        \n",
    "        exponents = quadratics + logdet + logconstant + logpi\n",
    "        old_logprobs = tf.reduce_logsumexp(exponents,axis=1)\n",
    "        \n",
    "        self.logp_old = old_logprobs\n",
    "    \n",
    "    def _kl_entropy(self):\n",
    "\n",
    "        def energy(mu1,std1,pi1,mu2,std2,pi2):\n",
    "            energy_components = []\n",
    "            for i in range(self.n_mixture):\n",
    "                for j in range(self.n_mixture):\n",
    "                    mu1i = tf.squeeze(mu1[:,:,i]) \n",
    "                    mu2j = tf.squeeze(mu2[:,:,j])\n",
    "                    std1i = tf.squeeze(std1[:,:,i])\n",
    "                    std2j = tf.squeeze(std2[:,:,j])\n",
    "                    pi1i = pi1[:,i]\n",
    "                    pi2j = pi2[:,j]\n",
    "                    energy_components.append(pi1i*pi2j * tf.exp(-0.5*tf.reduce_sum(((mu1i - mu2j)/(std1i+std2j))**2+2.*tf.log(std1i+std2j)+np.log(2*np.pi),axis=1)))\n",
    "            return tf.reduce_sum(tf.stack(energy_components),axis=1) \n",
    "            \n",
    "        mean, std, weight = self.means, self.std + 1e-10, self.pi + 1e-10\n",
    "        old_mean, old_std, old_weight = self.old_means_ph, self.old_std_ph + 1e-10, self.old_pi_ph + 1e-10\n",
    "\n",
    "        if self.mdn_weight==\"softmax\":\n",
    "            self.entropy = tf.reduce_sum(self.pi*(-tf.log(self.pi) + 0.5 * (self.act_dim * (np.log(2 * np.pi) + 1) +\n",
    "                                                                        tf.reduce_sum(tf.log(std),axis=1))),axis=1)\n",
    "            self.entropy = tf.reduce_mean(self.entropy)\n",
    "        elif self.mdn_weight==\"sparsemax\":\n",
    "            self.entropy = tf.reduce_mean(0.5*(1-energy(mean, std, weight,mean, std, weight)))\n",
    "            \n",
    "        log_det_cov_old = tf.reduce_sum(tf.log(old_std),axis=1)\n",
    "        log_det_cov_new = tf.reduce_sum(tf.log(std),axis=1)\n",
    "        tr_old_new = tf.reduce_sum(old_std/std,axis=1)\n",
    "\n",
    "        kl = tf.reduce_sum(self.old_pi_ph*tf.log(self.old_pi_ph/self.pi) + 0.5 * self.old_pi_ph*(log_det_cov_new - log_det_cov_old + tr_old_new +\n",
    "                         tf.reduce_sum(tf.square((self.means - self.old_means_ph)/std),axis=1) - self.act_dim),axis=1)\n",
    "        self.kl = tf.reduce_mean(kl)\n",
    "        \n",
    "    def _loss_train_op(self):\n",
    "        \n",
    "        loss1 = -tf.reduce_mean(self.advantages_ph *\n",
    "                                tf.exp(self.logp - self.logp_old))\n",
    "        loss2 = tf.reduce_mean(self.beta_ph * self.kl)\n",
    "        loss3 = self.eta_ph * tf.square(tf.maximum(0.0, self.kl - 2.0 * self.kl_targ))\n",
    "        self.loss = loss1 - self.entcoeff*self.entropy + loss2 + loss3\n",
    "        self.loss_p = -tf.reduce_mean(self.logp)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.lr_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        self.train_p_op = optimizer.minimize(self.loss_p)\n",
    "\n",
    "    def _init_session(self):\n",
    "        \"\"\"Launch TensorFlow session and initialize variables\"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config,graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        \"\"\"Draw sample from policy distribution\"\"\"\n",
    "        feed_dict = {self.obs_ph: obs, self.eps_ph:self.eps}\n",
    "        pi, mu, sigma = self.sess.run([self.pi, self.means, self.std],feed_dict=feed_dict)\n",
    "        pi = (pi+1e-10)/np.sum(pi+1e-10,axis=1,keepdims=True)\n",
    "        sigma = sigma+1e-10\n",
    "        n_points = np.shape(obs)[0]\n",
    "        \n",
    "        _y_sampled = np.zeros([n_points,self.act_dim])\n",
    "        for i in range(n_points):\n",
    "            k = np.random.choice(self.n_mixture,p=pi[i,:])\n",
    "            _y_sampled[i,:] = mu[i,:,k] + np.random.randn(1,self.act_dim)*sigma[i,:,k]\n",
    "        return _y_sampled\n",
    "    \n",
    "    def update(self, observes, actions, advantages, batch_size = 128):\n",
    "        \n",
    "        num_batches = max(observes.shape[0] // batch_size, 1)\n",
    "        batch_size = observes.shape[0] // num_batches\n",
    "        \n",
    "        old_means_np, old_std_np, old_pi_np = self.sess.run([self.means, self.std, self.pi],{self.obs_ph: observes, self.eps_ph:self.eps})\n",
    "        loss, kl, entropy = 0, 0, 0\n",
    "        for e in range(self.epochs):\n",
    "            # TODO: need to improve data pipeline - re-feeding data every epoch\n",
    "            observes, actions, advantages, old_means_np, old_std_np, old_pi_np = shuffle(observes, actions, advantages, old_means_np, old_std_np, old_pi_np, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: observes[start:end,:],\n",
    "                     self.act_ph: actions[start:end,:],\n",
    "                     self.advantages_ph: advantages[start:end],\n",
    "                     self.old_std_ph: old_std_np[start:end,:,:],\n",
    "                     self.old_means_ph: old_means_np[start:end,:,:],\n",
    "                     self.old_pi_ph: old_pi_np[start:end,:],\n",
    "                     self.beta_ph: self.beta,\n",
    "                     self.eta_ph: self.eta,\n",
    "                     self.lr_ph: self.lr * self.lr_multiplier,\n",
    "                     self.eps_ph: self.eps}        \n",
    "                self.sess.run(self.train_op, feed_dict)\n",
    "            \n",
    "            feed_dict = {self.obs_ph: observes,\n",
    "                 self.act_ph: actions,\n",
    "                 self.advantages_ph: advantages,\n",
    "                 self.old_std_ph: old_std_np,\n",
    "                 self.old_means_ph: old_means_np,\n",
    "                 self.old_pi_ph: old_pi_np,\n",
    "                 self.beta_ph: self.beta,\n",
    "                 self.eta_ph: self.eta,\n",
    "                 self.lr_ph: self.lr * self.lr_multiplier,\n",
    "                 self.eps_ph: self.eps}        \n",
    "            loss, kl, entropy = self.sess.run([self.loss, self.kl, self.entropy], feed_dict)\n",
    "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
    "                break\n",
    "                \n",
    "        # TODO: too many \"magic numbers\" in next 8 lines of code, need to clean up\n",
    "        if kl > self.kl_targ * 2:  # servo beta to reach D_KL target\n",
    "            self.beta = np.minimum(35, 1.5 * self.beta)  # max clip beta\n",
    "            if self.beta > 30 and self.lr_multiplier > 0.1:\n",
    "                self.lr_multiplier /= 1.5\n",
    "        elif kl < self.kl_targ / 2:\n",
    "            self.beta = np.maximum(1 / 35, self.beta / 1.5)  # min clip beta\n",
    "            if self.beta < (1 / 30) and self.lr_multiplier < 10:\n",
    "                self.lr_multiplier *= 1.5\n",
    "        \n",
    "        loss, kl, entropy = self.sess.run([self.loss, self.kl, self.entropy], feed_dict)\n",
    "        return loss, kl, entropy\n",
    "    \n",
    "    def update_eps(self, decaying_rate = 0.99):\n",
    "        self.eps *= decaying_rate\n",
    "    \n",
    "    def fit(self, x, y, batch_size=128):\n",
    "        \"\"\" Fit model to current data batch + previous data batch\n",
    "        Args:\n",
    "            x: features\n",
    "            y: target\n",
    "            logger: logger to save training loss and % explained variance\n",
    "        \"\"\"\n",
    "        num_batches = max(x.shape[0] // batch_size, 1)\n",
    "        batch_size = x.shape[0] // num_batches\n",
    "        \n",
    "        x_train = x\n",
    "        y_train = y\n",
    "        for e in range(self.epochs):\n",
    "            x_train, y_train = shuffle(x_train, y_train, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: x_train[start:end, :],\n",
    "                             self.act_ph: y_train[start:end, :],\n",
    "                             self.lr_ph: self.lr,\n",
    "                             self.eps_ph: 0.0}\n",
    "                _, l = self.sess.run([self.train_p_op, self.loss_p], feed_dict=feed_dict)\n",
    "        feed_dict = {self.obs_ph: x_train, self.act_ph: y_train, self.lr_ph: self.lr, self.eps_ph: 0.0}\n",
    "        loss_p = self.sess.run(self.loss_p, feed_dict=feed_dict)\n",
    "        return loss_p\n",
    "    \n",
    "    def save_policy(self, path):\n",
    "        saver = tf.train.Saver(self.variables)\n",
    "        saver.save(self.sess, path)\n",
    "    \n",
    "    def restore_session(self, path):\n",
    "        with self.g.as_default():\n",
    "            saver = tf.train.import_meta_graph(path+\".meta\")\n",
    "            saver.restore(self.sess, path)\n",
    "            \n",
    "    def close_sess(self):\n",
    "        \"\"\" Close TensorFlow session \"\"\"\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(object):\n",
    "    def __init__(self, obs_dim, epochs=20, lr=1e-3,seed=0):\n",
    "        self.seed = seed\n",
    "        self.replay_buffer_x = None\n",
    "        self.replay_buffer_y = None\n",
    "    \n",
    "        self.obs_dim = obs_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self._build_graph()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\" Construct TensorFlow graph, including loss function, init op and train op \"\"\"\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs_valfunc')\n",
    "            self.val_ph = tf.placeholder(tf.float32, (None,), 'val_valfunc')\n",
    "            \n",
    "            hid1_size = 64\n",
    "            hid2_size = 64\n",
    "            \n",
    "            out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\")\n",
    "            out = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output')\n",
    "            self.out = tf.squeeze(out)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.out - self.val_ph))\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "        self.sess = tf.Session(graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def fit(self, x, y, batch_size=32):\n",
    "        num_batches = max(x.shape[0] // batch_size, 1)\n",
    "        y_hat = self.predict(x)  # check explained variance prior to update\n",
    "        \n",
    "        if self.replay_buffer_x is None:\n",
    "            x_train, y_train = x, y\n",
    "        else:\n",
    "            x_train = np.concatenate([x, self.replay_buffer_x])\n",
    "            y_train = np.concatenate([y, self.replay_buffer_y])\n",
    "        self.replay_buffer_x = x\n",
    "        self.replay_buffer_y = y\n",
    "        for e in range(self.epochs):\n",
    "            x_train, y_train = shuffle(x_train, y_train, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: x_train[start:end, :],\n",
    "                             self.val_ph: y_train[start:end]}\n",
    "                _, l = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "        y_hat = self.predict(x)\n",
    "        loss = np.mean(np.square(y_hat - y))         # explained variance after update\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Predict method \"\"\"\n",
    "        feed_dict = {self.obs_ph: x}\n",
    "        y_hat = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "        return np.squeeze(y_hat)\n",
    "\n",
    "    def save_value(self, path):\n",
    "        saver = tf.train.Saver(self.variables)\n",
    "        saver.save(self.sess, path)\n",
    "        \n",
    "    def restore_session(self, path):\n",
    "        with self.g.as_default():\n",
    "            saver = tf.train.import_meta_graph(path+\".meta\")\n",
    "            saver.restore(self.sess, path)\n",
    "            \n",
    "    def close_sess(self):\n",
    "        \"\"\" Close TensorFlow session \"\"\"\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward(object):\n",
    "    def __init__(self, obs_dim, act_dim, epochs=20, lr=1e-3, entcoeff=1e-3, seed=0):\n",
    "        self.seed = seed\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.epochs = epochs\n",
    "        self.entcoeff = entcoeff\n",
    "        self.lr = lr  # learning rate set in _build_graph()\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\" Construct TensorFlow graph, including loss function, init op and train op \"\"\"\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.obs_act_exp_ph = tf.placeholder(tf.float32, (None, self.obs_dim + self.act_dim), 'obs_act_exp_rewfunc')\n",
    "            self.obs_act_gen_ph = tf.placeholder(tf.float32, (None, self.obs_dim + self.act_dim), 'obs_act_gen_rewfunc')\n",
    "            \n",
    "            hid1_size = 64  # 10 chosen empirically on 'Hopper-v1'\n",
    "            hid2_size = 64  # 10 chosen empirically on 'Hopper-v1'\n",
    "            \n",
    "            # 3 hidden layers with tanh activations\n",
    "            out = tf.layers.dense(self.obs_act_exp_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\")\n",
    "            exp_logits = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output')\n",
    "            self.exp_logits = tf.squeeze(exp_logits)\n",
    "            \n",
    "            out = tf.layers.dense(self.obs_act_gen_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\", reuse=True)\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\", reuse=True)\n",
    "            gen_logits = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output', reuse=True)\n",
    "            self.gen_logits = tf.squeeze(gen_logits)\n",
    "\n",
    "            self.generator_acc = tf.reduce_mean(tf.to_float(tf.nn.sigmoid(gen_logits) < 0.5))\n",
    "            self.expert_acc = tf.reduce_mean(tf.to_float(tf.nn.sigmoid(exp_logits) > 0.5))\n",
    "            \n",
    "            generator_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=gen_logits, labels=tf.zeros_like(gen_logits))\n",
    "            generator_loss = tf.reduce_mean(generator_loss)\n",
    "            expert_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=exp_logits, labels=tf.ones_like(exp_logits))\n",
    "            expert_loss = tf.reduce_mean(expert_loss)\n",
    "            \n",
    "            logits = tf.concat([gen_logits, exp_logits], 0)\n",
    "            entropy = tf.reduce_mean(-tf.nn.sigmoid(logits)*logits - tf.log(1-tf.nn.sigmoid(logits))) \n",
    "            entropy_loss = -self.entcoeff*entropy\n",
    "            # Loss + Accuracy terms\n",
    "            \n",
    "            self.loss = generator_loss + expert_loss + entropy_loss\n",
    "            # Build Reward for policy\n",
    "            self.reward = -tf.log(1-tf.nn.sigmoid(gen_logits) + 1e-10)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "        self.sess = tf.Session(graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def fit(self, obs_act_exp, obs_act_gen, batch_size=128):\n",
    "        data_size = min(obs_act_exp.shape[0],obs_act_gen.shape[0])\n",
    "        num_batches = max(data_size // batch_size, 1)\n",
    "        batch_size = data_size // num_batches\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            obs_act_exp = shuffle(obs_act_exp, random_state=self.seed)\n",
    "            obs_act_gen = shuffle(obs_act_gen, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_act_gen_ph:obs_act_gen[start:end,:], self.obs_act_exp_ph:obs_act_exp[start:end,:]}\n",
    "                self.sess.run(self.train_op, feed_dict=feed_dict)\n",
    "        feed_dict = {self.obs_act_gen_ph:obs_act_gen, self.obs_act_exp_ph:obs_act_exp}\n",
    "        loss, gen_acc, exp_acc = self.sess.run([self.loss,self.generator_acc,self.expert_acc], feed_dict=feed_dict)\n",
    "        return loss, gen_acc, exp_acc\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" Predict method \"\"\"\n",
    "        feed_dict = {self.obs_act_gen_ph: x}\n",
    "        rew_hat = self.sess.run(self.reward, feed_dict=feed_dict)\n",
    "\n",
    "        return np.squeeze(rew_hat)\n",
    "\n",
    "    def save_reward(self, path):\n",
    "        saver = tf.train.Saver(self.variables)\n",
    "        saver.save(self.sess, path)\n",
    "    \n",
    "    def restore_session(self, path):\n",
    "        with self.g.as_default():\n",
    "            saver = tf.train.import_meta_graph(path+\".meta\")\n",
    "            saver.restore(self.sess, path)\n",
    "            \n",
    "    def close_sess(self):\n",
    "        \"\"\" Close TensorFlow session \"\"\"\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    \"\"\" Calculate discounted forward sum of a sequence at each point \"\"\"\n",
    "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
    "\n",
    "\n",
    "def add_disc_sum_rew(trajectories, gamma):\n",
    "    \"\"\" Adds discounted sum of rewards to all time steps of all trajectories\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy()\n",
    "        gamma: discount\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'disc_sum_rew')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        disc_sum_rew = discount(rewards, gamma)\n",
    "        trajectory['disc_sum_rew'] = disc_sum_rew\n",
    "\n",
    "def add_rew(trajectories, rew_func):\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        actions = trajectory['actions']\n",
    "        observes_actions = np.concatenate([observes,actions],axis=1)\n",
    "        trajectory['rewards'] = rew_func.predict(observes_actions)\n",
    "    return trajectories\n",
    "\n",
    "def add_value(trajectories, val_func):\n",
    "    \"\"\" Adds estimated value to all time steps of all trajectories\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy()\n",
    "        val_func: object with predict() method, takes observations\n",
    "            and returns predicted state value\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'values')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = val_func.predict(observes)\n",
    "        trajectory['values'] = values\n",
    "\n",
    "def add_gae(trajectories, gamma, lam):\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        values = trajectory['values']\n",
    "        # temporal differences\n",
    "        tds = rewards - values + np.append(values[1:] * gamma, 0)\n",
    "        advantages = discount(tds, gamma * lam)\n",
    "        trajectory['advantages'] = advantages\n",
    "\n",
    "def build_train_set(trajectories):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        trajectories: trajectories after processing by add_disc_sum_rew(),\n",
    "            add_value(), and add_gae()\n",
    "    Returns: 4-tuple of NumPy arrays\n",
    "        observes: shape = (N, obs_dim)\n",
    "        actions: shape = (N, act_dim)\n",
    "        advantages: shape = (N,)\n",
    "        disc_sum_rew: shape = (N,)\n",
    "    \"\"\"\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    disc_sum_rew = np.concatenate([t['disc_sum_rew'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "    # normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "\n",
    "    return observes, actions, advantages, disc_sum_rew\n",
    "\n",
    "def build_train_set_for_rew(trajectories,demonstrations):\n",
    "    \n",
    "    real_observes = np.concatenate([d['observes'] for d in demonstrations])\n",
    "    real_actions = np.concatenate([d['actions'] for d in demonstrations])\n",
    "    obs_act_exp = np.concatenate([real_observes,real_actions],axis=1)\n",
    "    \n",
    "    fake_observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    fake_actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    obs_act_gen = np.concatenate([fake_observes,fake_actions],axis=1)\n",
    "    return obs_act_exp, obs_act_gen\n",
    "\n",
    "def run_episode(env, policy, animate=False):\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, infos = [], [], [], []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if animate:\n",
    "            env.render()\n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        observes.append(obs)\n",
    "        action = policy.sample(obs).reshape((1, -1)).astype(np.float32)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if not isinstance(reward, float):\n",
    "            reward = np.asscalar(reward)\n",
    "        rewards.append(reward)\n",
    "        infos.append(info)\n",
    "        \n",
    "    return (np.concatenate(observes), np.concatenate(actions),\n",
    "            np.array(rewards, dtype=np.float32), infos)\n",
    "\n",
    "def run_policy(env, policy, episodes):\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, infos = run_episode(env, policy)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes,\n",
    "                      'actions': actions,\n",
    "                      'true_rewards': rewards,\n",
    "                      'infos': infos}\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "\n",
    "def evaluation(env, policy, max_eval_epi=100, seed=0):\n",
    "    return_list = np.zeros((max_eval_epi,))\n",
    "    info_list = []\n",
    "\n",
    "    env.seed(seed)\n",
    "    for epi in range(max_eval_epi):\n",
    "        obs = env.reset()\n",
    "        env_infos_epi_list = {\"pos\": [],\"goal_id\": []}\n",
    "        observes, actions, rewards, infos = run_episode(env, policy)\n",
    "\n",
    "        pos_list = []\n",
    "        goal_id_list = []\n",
    "        for info in infos:\n",
    "            pos_list.append(info[\"pos\"])\n",
    "            goal_id_list.append(info[\"goal_id\"])\n",
    "        env_infos_epi_list[\"pos\"] = np.asarray(pos_list)\n",
    "        env_infos_epi_list[\"goal_id\"] = np.asarray(goal_id_list)\n",
    "\n",
    "        info_list.append({\"env_infos\":env_infos_epi_list})\n",
    "        return_list[epi] = np.sum(rewards)\n",
    "#     print(\"Evaluation Result: {}\".format(np.mean(return_list)))\n",
    "    return return_list, info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/seed:0,kl:3.00e-03,entcoeff:1.00e-03,weight:sparsemax,mixture:4,epi_size:500_policy.ckpt\n",
      "./results/seed:0,kl:3.00e-03,entcoeff:1.00e-03,weight:sparsemax,mixture:4,epi_size:500_reward.ckpt\n",
      "./results/seed:0,kl:3.00e-03,entcoeff:1.00e-03,weight:sparsemax,mixture:4,epi_size:500_value.ckpt\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "\n",
    "demo_file = open('./multigoal_expert_demo.pkl', 'r')\n",
    "demonstrations, = pickle.load(demo_file)\n",
    "demonstrations = shuffle(demonstrations,random_state=seed)[:400]\n",
    "\n",
    "demo_observes = []\n",
    "demo_actions = []\n",
    "for demonstration in demonstrations:\n",
    "    for obs in demonstration['observes']:\n",
    "        demo_observes.append(obs)\n",
    "    for act in demonstration['actions']:\n",
    "        demo_actions.append(act)\n",
    "demo_observes=np.asarray(demo_observes)\n",
    "demo_actions=np.asarray(demo_actions)\n",
    "exp_ret = np.mean([np.sum(t['rewards']) for t in demonstrations])\n",
    "\n",
    "env = multigoal.MultiGoalEnv(nr_goal=4)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "gamma = 0.995\n",
    "lam = 0.98\n",
    "kl_targ = 0.003\n",
    "entcoeff = 1e-3\n",
    "mdn_weight = \"sparsemax\"\n",
    "n_mixture = 4\n",
    "max_std=0.5\n",
    "episode_size = 500    \n",
    "batch_size = 1024\n",
    "nupdates = 600\n",
    "\n",
    "saver_prefix=\"./results/seed:{},kl:{:.2e},entcoeff:{:.2e},weight:{},mixture:{:d},epi_size:{}\".format(seed,kl_targ,entcoeff,mdn_weight,n_mixture,episode_size)\n",
    "print(saver_prefix+\"_policy.ckpt\")\n",
    "print(saver_prefix+\"_reward.ckpt\")\n",
    "print(saver_prefix+\"_value.ckpt\")\n",
    "\n",
    "policy = Policy(obs_dim, act_dim,kl_targ=kl_targ,entcoeff=entcoeff,mdn_weight=mdn_weight,n_mixture=n_mixture,max_std=max_std,eps=0.0,seed=seed)\n",
    "val_func = Value(obs_dim,seed=seed)\n",
    "rew_func = Reward(obs_dim,act_dim,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./results/seed:0,kl:3.00e-03,entcoeff:1.00e-03,weight:sparsemax,mixture:4,epi_size:500_policy.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-25 17:52:19,932] Restoring parameters from ./results/seed:0,kl:3.00e-03,entcoeff:1.00e-03,weight:sparsemax,mixture:4,epi_size:500_policy.ckpt\n"
     ]
    }
   ],
   "source": [
    "policy.restore_session(saver_prefix+\"_policy.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
